#!/usr/bin/env python3
"""
LangChain RAG-based Vulnerability Classifier
Enriches vulnerability data with CWE, MITRE ATT&CK, and RL agent hints
"""

import json
import os
import time
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
from pathlib import Path

# LangChain imports (optional - only if RAG mode is enabled)
try:
    from langchain_openai import ChatOpenAI  # type: ignore
    from langchain_core.prompts import ChatPromptTemplate  # type: ignore
    from langchain_core.output_parsers import JsonOutputParser  # type: ignore
    from dotenv import load_dotenv  # type: ignore
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    ChatOpenAI = None  # type: ignore
    ChatPromptTemplate = None  # type: ignore
    JsonOutputParser = None  # type: ignore
    def load_dotenv(): pass  # type: ignore

# Import from same directory (Classifier/)
import sys
import os
sys.path.insert(0, os.path.dirname(__file__))
from knowledge_base import KnowledgeBase, get_knowledge_base  # type: ignore


class VulnerabilityClassifier:
    """
    Hybrid vulnerability classifier using pattern matching, CVE lookup, and optional RAG
    """
    
    VERSION = "1.0.0"
    
    def __init__(
        self,
        mode: str = "hybrid",
        enable_rag: bool = False,
        confidence_threshold: float = 0.7,
        api_key: Optional[str] = None
    ):
        """
        Initialize the vulnerability classifier
        
        Args:
            mode: Classification mode ("pattern", "hybrid", "rag")
            enable_rag: Enable RAG-based classification for complex cases
            confidence_threshold: Minimum confidence for pattern matching
            api_key: OpenRouter API key (reads from env if not provided)
        """
        self.mode = mode
        self.enable_rag = enable_rag and LANGCHAIN_AVAILABLE
        self.confidence_threshold = confidence_threshold
        
        # Load environment variables
        load_dotenv()
        
        # Initialize knowledge base
        self.kb = get_knowledge_base()
        
        # Initialize LLM if RAG is enabled
        self.llm = None
        if self.enable_rag:
            if not LANGCHAIN_AVAILABLE:
                print("WARNING: LangChain not available. Falling back to pattern matching.")
                self.enable_rag = False
            else:
                self._init_llm(api_key)
        
        # Stats tracking
        self.stats = {
            "total_classified": 0,
            "pattern_matches": 0,
            "cve_lookups": 0,
            "rag_classifications": 0,
            "fallback_generic": 0
        }
    
    def _init_llm(self, api_key: Optional[str] = None):
        """Initialize LLM for RAG-based classification"""
        key = api_key or os.getenv("OPENROUTER_API_KEY")
        if not key:
            print("WARNING: No OpenRouter API key found. RAG mode disabled.")
            self.enable_rag = False
            return
        
        model = os.getenv("OPENROUTER_MODEL", "openrouter/auto")
        
        try:
            self.llm = ChatOpenAI(  # type: ignore
                api_key="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",  # type: ignore
                base_url="https://openrouter.ai/api/v1",
                model="openrouter/sherlock-think-alpha",
                temperature=0.1  # Low temperature for consistent classification
            )
        except Exception as e:
            print(f"WARNING: Failed to initialize LLM: {e}. RAG mode disabled.")
            self.enable_rag = False
    
    def classify_vulnerability(self, vuln: Dict[str, Any]) -> Dict[str, Any]:
        """
        Classify a single vulnerability
        
        Args:
            vuln: Vulnerability data dict (from parser)
        
        Returns:
            Enriched vulnerability with classification
        """
        start_time = time.time()
        
        # Extract key fields
        vuln_id = vuln.get("id", "unknown")
        plugin_name = vuln.get("pn", "")
        description = vuln.get("d", "")
        cve_id = vuln.get("c", "")
        cvss = vuln.get("cvss", 0.0)
        severity = vuln.get("s", 0)
        port = vuln.get("p", 0)
        host = vuln.get("h", "")
        
        # Try classification tiers
        classification = None
        source = "unknown"
        
        # Tier 1: CVE Lookup (if CVE exists)
        if cve_id and self.mode in ["hybrid", "cve"]:
            classification = self._classify_by_cve(cve_id, vuln)
            if classification:
                source = "cve_lookup"
                self.stats["cve_lookups"] += 1
        
        # Tier 2: Pattern Matching
        if not classification and self.mode in ["hybrid", "pattern"]:
            classification = self._classify_by_pattern(plugin_name, description, port)
            if classification and classification.get("confidence", 0) >= self.confidence_threshold:
                source = "pattern_match"
                self.stats["pattern_matches"] += 1
            else:
                classification = None  # Confidence too low
        
        # Tier 3: RAG-based Classification (if enabled and previous tiers failed)
        if not classification and self.enable_rag and self.mode in ["hybrid", "rag"]:
            classification = self._classify_by_rag(vuln)
            if classification:
                source = "rag_retrieval"
                self.stats["rag_classifications"] += 1
        
        # Fallback: Generic classification based on CVSS/severity
        if not classification:
            classification = self._generic_classification(vuln)
            source = "generic_fallback"
            self.stats["fallback_generic"] += 1
        
        # Enrich with RL agent hints
        classification = self._enrich_for_rl_agent(vuln, classification)
        
        # Add metadata
        classification["categorization_source"] = source
        
        # Calculate priority score
        priority_score = self._calculate_priority_score(vuln, classification)
        classification["priority_score"] = priority_score
        
        # Build final output
        processing_time = (time.time() - start_time) * 1000  # ms
        
        result = {
            "id": vuln_id,
            "original": vuln,
            "classification": classification,
            "metadata": {
                "classified_at": datetime.utcnow().isoformat() + "Z",
                "classifier_version": self.VERSION,
                "processing_time_ms": round(processing_time, 2)
            }
        }
        
        self.stats["total_classified"] += 1
        
        return result
    
    def _classify_by_cve(self, cve_id: str, vuln: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Classify using CVE lookup"""
        cve_data = self.kb.lookup_cve(cve_id)
        if not cve_data:
            return None
        
        return {
            "cwe": cve_data.get("cwe", []),
            "cwe_names": [self._get_cwe_name(cwe) for cwe in cve_data.get("cwe", [])],
            "mitre_attack": cve_data.get("mitre_attack", {}),
            "confidence": 0.95,
            "exploitation_assessment": {
                "difficulty": "Medium",
                "attack_vector": "Network",
                "requires_auth": False,
                "publicly_available_exploit": True
            }
        }
    
    def _classify_by_pattern(
        self, 
        plugin_name: str, 
        description: str, 
        port: int
    ) -> Optional[Dict[str, Any]]:
        """Classify using pattern matching"""
        # Match patterns
        matches = self.kb.match_pattern(description, plugin_name)
        
        if not matches:
            return None
        
        # Use best match
        best_match = matches[0]
        pattern_data = best_match["pattern_data"]
        
        return {
            "cwe": pattern_data.get("cwe", []),
            "cwe_names": pattern_data.get("cwe_names", []),
            "mitre_attack": pattern_data.get("mitre_attack", {}),
            "confidence": best_match["confidence"],
            "exploitation_assessment": {
                "difficulty": pattern_data.get("exploitation_difficulty", "Medium"),
                "attack_vector": "Network",
                "requires_auth": False,
                "publicly_available_exploit": pattern_data.get("exploitation_difficulty") == "Low"
            }
        }
    
    def _classify_by_rag(self, vuln: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Classify using RAG (Retrieval Augmented Generation)
        
        NOTE: This is a simplified implementation. Full RAG would:
        1. Retrieve similar CWE/ATT&CK entries from vector store
        2. Use LLM to synthesize classification based on retrieved context
        """
        if not self.llm:
            return None
        
        # Build prompt with vulnerability context
        prompt = ChatPromptTemplate.from_messages([  # type: ignore
            ("system", """You are a cybersecurity expert specializing in vulnerability classification.
Categorize the following vulnerability for a penetration testing RL agent.

Respond with JSON containing:
- cwe: list of CWE IDs (e.g., ["CWE-89", "CWE-78"])
- cwe_names: list of CWE names
- mitre_attack: dict with tactics (list), techniques (list), technique_names (list)
- confidence: float 0-1
- exploitation_assessment: dict with difficulty (Low/Medium/High), attack_vector, requires_auth (bool), publicly_available_exploit (bool)

Be precise and conservative. If unsure, use generic categories."""),
            ("human", """Vulnerability:
- Name: {plugin_name}
- CVE: {cve}
- CVSS: {cvss}
- Description: {description}

Classify this vulnerability.""")
        ])
        
        try:
            chain = prompt | self.llm | JsonOutputParser()  # type: ignore
            result = chain.invoke({
                "plugin_name": vuln.get("pn", "Unknown"),
                "cve": vuln.get("c", "None"),
                "cvss": vuln.get("cvss", 0),
                "description": vuln.get("d", "")[:500]  # Limit description length
            })
            
            return result
        except Exception as e:
            print(f"WARNING: RAG classification failed: {e}")
            return None
    
    def _generic_classification(self, vuln: Dict[str, Any]) -> Dict[str, Any]:
        """Fallback generic classification based on severity and CVSS"""
        cvss = vuln.get("cvss", 0.0)
        severity = vuln.get("s", 0)
        
        # Generic categories based on severity
        if severity >= 4:
            cwe = ["CWE-1035"]  # Generic: 2011 Top 25
            cwe_names = ["Critical Weakness"]
        elif severity >= 3:
            cwe = ["CWE-699"]  # Generic: Software Development
            cwe_names = ["High Severity Weakness"]
        else:
            cwe = ["CWE-1008"]  # Generic: Architectural Concepts
            cwe_names = ["Medium/Low Severity Weakness"]
        
        return {
            "cwe": cwe,
            "cwe_names": cwe_names,
            "mitre_attack": {
                "tactics": ["Initial Access"],
                "techniques": ["T1190"],
                "technique_names": ["Exploit Public-Facing Application"]
            },
            "confidence": 0.3,  # Low confidence for generic classification
            "exploitation_assessment": {
                "difficulty": "Unknown",
                "attack_vector": "Network",
                "requires_auth": False,
                "publicly_available_exploit": False
            }
        }
    
    def _enrich_for_rl_agent(
        self, 
        vuln: Dict[str, Any], 
        classification: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Add RL agent-specific hints and guidance"""
        # Get pattern data if available for tool suggestions
        description = vuln.get("d", "")
        plugin_name = vuln.get("pn", "")
        matches = self.kb.match_pattern(description, plugin_name)
        
        suggested_tools = ["nmap", "metasploit"]
        attack_type = "generic_exploitation"
        validation_strategy = "Verify vulnerability through version detection or safe exploitation"
        expected_impact = "Potential system compromise"
        next_steps = ["enumerate_version", "search_exploits", "test_exploit"]
        
        if matches:
            pattern_data = matches[0]["pattern_data"]
            suggested_tools = pattern_data.get("suggested_tools", suggested_tools)
            attack_type = pattern_data.get("attack_type", attack_type)
        
        # Customize based on vulnerability type
        cvss = vuln.get("cvss", 0.0)
        if cvss >= 9.0:
            expected_impact = "Critical - Remote Code Execution or Full System Compromise"
            next_steps.insert(0, "prioritize_immediate_testing")
        
        classification["rl_agent_hints"] = {
            "attack_type": attack_type,
            "suggested_tools": suggested_tools,
            "validation_strategy": validation_strategy,
            "expected_impact": expected_impact,
            "next_steps": next_steps
        }
        
        return classification
    
    def _calculate_priority_score(
        self, 
        vuln: Dict[str, Any], 
        classification: Dict[str, Any]
    ) -> float:
        """
        Calculate priority score for RL agent (0-10)
        
        Factors:
        - CVSS score (base)
        - Exploitation difficulty (easier = higher priority)
        - Public exploit availability
        - Confidence in classification
        """
        cvss = vuln.get("cvss", 0.0)
        
        # Start with normalized CVSS
        score = cvss
        
        # Adjust for exploitation difficulty
        difficulty = classification.get("exploitation_assessment", {}).get("difficulty", "Medium")
        if difficulty == "Low":
            score += 1.0
        elif difficulty == "High":
            score -= 1.0
        
        # Boost if public exploit available
        if classification.get("exploitation_assessment", {}).get("publicly_available_exploit"):
            score += 0.5
        
        # Adjust for classification confidence
        confidence = classification.get("confidence", 0.5)
        score *= (0.7 + 0.3 * confidence)  # 70-100% of score based on confidence
        
        # Apply pattern-specific modifiers
        description = vuln.get("d", "")
        plugin_name = vuln.get("pn", "")
        matches = self.kb.match_pattern(description, plugin_name)
        if matches:
            pattern_data = matches[0]["pattern_data"]
            modifier = pattern_data.get("priority_modifier", 1.0)
            score *= modifier
        
        # Clamp to 0-10
        return min(10.0, max(0.0, score))
    
    def _get_cwe_name(self, cwe_id: str) -> str:
        """Get human-readable name for CWE ID"""
        # This is a simplified mapping. Full implementation would have complete CWE database
        cwe_names = {
            "CWE-89": "SQL Injection",
            "CWE-78": "OS Command Injection",
            "CWE-79": "Cross-site Scripting",
            "CWE-94": "Improper Control of Generation of Code",
            "CWE-119": "Improper Restriction of Operations within Memory Buffers",
            "CWE-120": "Buffer Copy without Checking Size of Input",
            "CWE-200": "Exposure of Sensitive Information",
            "CWE-287": "Improper Authentication",
            "CWE-306": "Missing Authentication for Critical Function",
            "CWE-327": "Use of Broken Cryptographic Algorithm",
            "CWE-338": "Use of Cryptographically Weak PRNG",
            "CWE-521": "Weak Password Requirements",
            "CWE-798": "Use of Hard-coded Credentials",
            "CWE-912": "Hidden Functionality",
            "CWE-1104": "Use of Unmaintained Third Party Components"
        }
        return cwe_names.get(cwe_id, cwe_id)
    
    def classify_batch(self, vulnerabilities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Classify multiple vulnerabilities
        
        Args:
            vulnerabilities: List of vulnerability dicts
        
        Returns:
            List of classified vulnerabilities
        """
        results = []
        total = len(vulnerabilities)
        
        for i, vuln in enumerate(vulnerabilities, 1):
            if i % 10 == 0 or i == total:
                print(f"Classifying: {i}/{total}")
            
            result = self.classify_vulnerability(vuln)
            results.append(result)
        
        return results
    
    def get_stats(self) -> Dict[str, Any]:
        """Get classification statistics"""
        return self.stats.copy()


def classify_from_file(
    input_file: str,
    output_file: Optional[str] = None,
    mode: str = "hybrid",
    enable_rag: bool = False
) -> List[Dict[str, Any]]:
    """
    Convenience function to classify vulnerabilities from JSON file
    
    Args:
        input_file: Path to input JSON file (parser output)
        output_file: Optional path to save enriched output
        mode: Classification mode
        enable_rag: Enable RAG-based classification
    
    Returns:
        List of classified vulnerabilities
    """
    # Load input
    with open(input_file, 'r') as f:
        vulnerabilities = json.load(f)
    
    # Ensure it's a list
    if isinstance(vulnerabilities, dict):
        # Handle full format with metadata
        if "vulnerabilities" in vulnerabilities:
            # Extract all vulnerabilities from severity buckets
            all_vulns = []
            for severity in ["critical", "high", "medium", "low", "info"]:
                all_vulns.extend(vulnerabilities["vulnerabilities"].get(severity, []))
            vulnerabilities = all_vulns
        else:
            vulnerabilities = [vulnerabilities]
    
    # Classify
    classifier = VulnerabilityClassifier(mode=mode, enable_rag=enable_rag)
    results = classifier.classify_batch(vulnerabilities)
    
    # Print stats
    print("\nClassification Stats:")
    for key, value in classifier.get_stats().items():
        print(f"  {key}: {value}")
    
    # Save if output file specified
    if output_file:
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\nSaved to: {output_file}")
    
    return results


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python vulnerability_classifier.py <input_file> [output_file]")
        print("\nExample:")
        print("  python vulnerability_classifier.py VA_Output/critical_ms2_scan.json classified_output.json")
        sys.exit(1)
    
    input_file = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else None
    
    results = classify_from_file(input_file, output_file, mode="hybrid", enable_rag=False)
    
    if not output_file:
        # Print first result as sample
        if results:
            print("\nSample classification:")
            print(json.dumps(results[0], indent=2))
